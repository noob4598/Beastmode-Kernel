<<<<<<< HEAD
zsmalloc Memory Allocator

Overview

zmalloc a new slab-based memory allocator,
zsmalloc, for storing compressed pages.  It is designed for
low fragmentation and high allocation success rate on
large object, but <= PAGE_SIZE allocations.

zsmalloc differs from the kernel slab allocator in two primary
ways to achieve these design goals.

zsmalloc never requires high order page allocations to back
slabs, or "size classes" in zsmalloc terms. Instead it allows
multiple single-order pages to be stitched together into a
"zspage" which backs the slab.  This allows for higher allocation
success rate under memory pressure.

Also, zsmalloc allows objects to span page boundaries within the
zspage.  This allows for lower fragmentation than could be had
with the kernel slab allocator for objects between PAGE_SIZE/2
and PAGE_SIZE.  With the kernel slab allocator, if a page compresses
to 60% of it original size, the memory savings gained through
compression is lost in fragmentation because another object of
the same size can't be stored in the leftover space.

This ability to span pages results in zsmalloc allocations not being
directly addressable by the user.  The user is given an
non-dereferencable handle in response to an allocation request.
That handle must be mapped, using zs_map_object(), which returns
a pointer to the mapped region that can be used.  The mapping is
necessary since the object data may reside in two different
noncontigious pages.

For 32-bit systems, zsmalloc has the added benefit of being
able to back slabs with HIGHMEM pages, something not possible
with the kernel slab allocators (SLAB or SLUB).

Usage:

#include <linux/zsmalloc.h>

/* create a new pool */
struct zs_pool *pool = zs_create_pool("mypool", GFP_KERNEL);

/* allocate a 256 byte object */
unsigned long handle = zs_malloc(pool, 256);

/*
 * Map the object to get a dereferenceable pointer in "read-write mode"
 * (see zsmalloc.h for additional modes)
 */
void *ptr = zs_map_object(pool, handle, ZS_MM_RW);

/* do something with ptr */

/*
 * Unmap the object when done dealing with it. You should try to
 * minimize the time for which the object is mapped since preemption
 * is disabled during the mapped period.
 */
zs_unmap_object(pool, handle);

/* free the object */
zs_free(pool, handle);

/* destroy the pool */
zs_destroy_pool(pool);
=======
zsmalloc
--------

This allocator is designed for use with zram. Thus, the allocator is
supposed to work well under low memory conditions. In particular, it
never attempts higher order page allocation which is very likely to
fail under memory pressure. On the other hand, if we just use single
(0-order) pages, it would suffer from very high fragmentation --
any object of size PAGE_SIZE/2 or larger would occupy an entire page.
This was one of the major issues with its predecessor (xvmalloc).

To overcome these issues, zsmalloc allocates a bunch of 0-order pages
and links them together using various 'struct page' fields. These linked
pages act as a single higher-order page i.e. an object can span 0-order
page boundaries. The code refers to these linked pages as a single entity
called zspage.

For simplicity, zsmalloc can only allocate objects of size up to PAGE_SIZE
since this satisfies the requirements of all its current users (in the
worst case, page is incompressible and is thus stored "as-is" i.e. in
uncompressed form). For allocation requests larger than this size, failure
is returned (see zs_malloc).

Additionally, zs_malloc() does not return a dereferenceable pointer.
Instead, it returns an opaque handle (unsigned long) which encodes actual
location of the allocated object. The reason for this indirection is that
zsmalloc does not keep zspages permanently mapped since that would cause
issues on 32-bit systems where the VA region for kernel space mappings
is very small. So, before using the allocating memory, the object has to
be mapped using zs_map_object() to get a usable pointer and subsequently
unmapped using zs_unmap_object().

stat
----

With CONFIG_ZSMALLOC_STAT, we could see zsmalloc internal information via
/sys/kernel/debug/zsmalloc/<user name>. Here is a sample of stat output:

# cat /sys/kernel/debug/zsmalloc/zram0/classes

 class  size almost_full almost_empty obj_allocated   obj_used pages_used pages_per_zspage
    ..
    ..
     9   176           0            1           186        129          8                4
    10   192           1            0          2880       2872        135                3
    11   208           0            1           819        795         42                2
    12   224           0            1           219        159         12                4
    ..
    ..


class: index
size: object size zspage stores
almost_empty: the number of ZS_ALMOST_EMPTY zspages(see below)
almost_full: the number of ZS_ALMOST_FULL zspages(see below)
obj_allocated: the number of objects allocated
obj_used: the number of objects allocated to the user
pages_used: the number of pages allocated for the class
pages_per_zspage: the number of 0-order pages to make a zspage

We assign a zspage to ZS_ALMOST_EMPTY fullness group when:
      n <= N / f, where
n = number of allocated objects
N = total number of objects zspage can store
f = fullness_threshold_frac(ie, 4 at the moment)

Similarly, we assign zspage to:
      ZS_ALMOST_FULL  when n > N / f
      ZS_EMPTY        when n == 0
      ZS_FULL         when n == N
>>>>>>> f1f997bb2aa14231c38c2cd423ac6da380356b03
